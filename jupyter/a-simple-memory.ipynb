{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dbdf009-e881-4664-a78d-92877650ce64",
   "metadata": {},
   "source": [
    "## From Chapter 4 - a-simple-memory.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e9d6b00-4423-4e65-b90f-3ac1e48faf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt = input_variables=[] optional_variables=['messages'] input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x73e21c3defc0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={'messages': []} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant. Answer all questions to the best of         your ability.'), additional_kwargs={}), MessagesPlaceholder(variable_name='messages', optional=True)]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of         your ability.\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "print(f\"prompt = {prompt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133526b-9039-49bd-818c-792b510c1f71",
   "metadata": {},
   "source": [
    "## Pretty Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e33457a4-67a8-4b02-8b67-5ed50dfb1d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template Structure:\n",
      "==================================================\n",
      "Type: ChatPromptTemplate\n",
      "Number of messages: 2\n",
      "\n",
      "Message 1:\n",
      "  Type: SystemMessagePromptTemplate\n",
      "  Role: system\n",
      "  Content: You are a helpful assistant. Answer all questions to the best of your ability.\n",
      "  Available attributes: ['additional_kwargs', 'aformat', 'aformat_messages', 'construct', 'copy', 'dict', 'format', 'format_messages', 'from_orm', 'from_template', 'from_template_file', 'get_lc_namespace', 'input_variables', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'pretty_print', 'pretty_repr', 'prompt', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'update_forward_refs', 'validate']\n",
      "\n",
      "Message 2:\n",
      "  Type: MessagesPlaceholder\n",
      "  Role: messagesplaceholder\n",
      "  Content: variable_name='messages' optional=True\n",
      "  Available attributes: ['aformat_messages', 'construct', 'copy', 'dict', 'format_messages', 'from_orm', 'get_lc_namespace', 'input_variables', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'n_messages', 'optional', 'parse_file', 'parse_obj', 'parse_raw', 'pretty_print', 'pretty_repr', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'update_forward_refs', 'validate', 'variable_name']\n",
      "\n",
      "Prompt Object:\n",
      "------------------------------\n",
      "input_variables=[] optional_variables=['messages'] input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x73e21c3defc0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={'messages': []} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant. Answer all questions to the best of your ability.'), additional_kwargs={}), MessagesPlaceholder(variable_name='messages', optional=True)]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create a chat prompt template with system message and placeholder for conversation\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "# Pretty print the prompt structure\n",
    "print(\"Prompt Template Structure:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Type: {type(prompt).__name__}\")\n",
    "print(f\"Number of messages: {len(prompt.messages)}\")\n",
    "\n",
    "# Display each message template\n",
    "for i, message in enumerate(prompt.messages):\n",
    "    print(f\"\\nMessage {i + 1}:\")\n",
    "    print(f\"  Type: {type(message).__name__}\")\n",
    "    \n",
    "    # Get the role from the message type or __class__ name\n",
    "    if hasattr(message, '__class__'):\n",
    "        role = message.__class__.__name__.replace('MessagePromptTemplate', '').lower()\n",
    "        print(f\"  Role: {role}\")\n",
    "    \n",
    "    # Try different ways to get the content\n",
    "    if hasattr(message, 'prompt'):\n",
    "        if hasattr(message.prompt, 'template'):\n",
    "            print(f\"  Content: {message.prompt.template}\")\n",
    "        else:\n",
    "            print(f\"  Content: {message.prompt}\")\n",
    "    elif hasattr(message, 'template'):\n",
    "        print(f\"  Content: {message.template}\")\n",
    "    else:\n",
    "        print(f\"  Content: {message}\")\n",
    "    \n",
    "    # Show all available attributes for debugging\n",
    "    print(f\"  Available attributes: {[attr for attr in dir(message) if not attr.startswith('_')]}\")\n",
    "\n",
    "# Alternative: Use the prompt's pretty representation\n",
    "print(\"\\nPrompt Object:\")\n",
    "print(\"-\" * 30)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9065b7-f529-4940-bc2f-57f11894109f",
   "metadata": {},
   "source": [
    "`ChatPromptTemplate.from_messages` is like a conversation blueprint builder in LangChain. Let me break it down:\n",
    "\n",
    "## What it does\n",
    "It creates a structured template for chat conversations by defining different types of messages and their roles. Think of it like creating a script template for a play - you define who says what and when.\n",
    "\n",
    "## How it works\n",
    "```python\n",
    "ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant...\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "```\n",
    "\n",
    "Each tuple in the list represents a message with two parts:\n",
    "- **Role** (first element): Who's \"speaking\" - system, user, assistant, placeholder\n",
    "- **Content** (second element): What they say or a template for what they'll say\n",
    "\n",
    "## The roles explained\n",
    "Think of it like different actors in a conversation:\n",
    "\n",
    "- **\"system\"**: The director's instructions - sets the AI's behavior and personality\n",
    "- **\"user\"**: The human's messages \n",
    "- **\"assistant\"**: The AI's responses\n",
    "- **\"placeholder\"**: A slot that gets filled with actual conversation history later\n",
    "\n",
    "## Your specific example\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "```\n",
    "\n",
    "This creates a template that:\n",
    "1. **Always starts** with system instructions (like giving the AI its job description)\n",
    "2. **Has a placeholder** `{messages}` that gets replaced with the actual back-and-forth conversation\n",
    "\n",
    "## The analogy\n",
    "It's like creating a form letter template:\n",
    "- The system message is like the letterhead that's always the same\n",
    "- The placeholder is like \"Dear {name}\" - it gets filled in with real data later\n",
    "\n",
    "When you use this template, LangChain will:\n",
    "1. Keep the system message at the top\n",
    "2. Replace `{messages}` with the actual conversation history\n",
    "3. Send the complete formatted conversation to the AI model\n",
    "\n",
    "This pattern is super common because it lets you maintain consistent AI behavior (via system message) while handling dynamic conversations (via the placeholder)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775e441-7eb9-4d63-b4a6-9a6cddc0afc7",
   "metadata": {},
   "source": [
    "## Get API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "782b9f06-dd16-4b5b-b7b1-dcd5360688ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using python-dotenv:\n",
      "API Key: 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\n",
      "Database URL: None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Method 1: Using python-dotenv (recommended)\n",
    "# First install: pip install python-dotenv\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    \n",
    "    # Load .env file from home directory\n",
    "    dotenv_path = Path.home() / '.env'\n",
    "    load_dotenv(dotenv_path)\n",
    "    \n",
    "    # Now you can access environment variables\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    database_url = os.getenv('DATABASE_URL')\n",
    "    \n",
    "    print(\"Using python-dotenv:\")\n",
    "    print(f\"API Key: {api_key}\")\n",
    "    print(f\"Database URL: {database_url}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed. Install with: pip install python-dotenv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d4cf9db-4cfb-4caa-996f-f55be0ac2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "\n",
    "chain = prompt | model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14389fd-2185-46a1-bac6-19d77c09cd50",
   "metadata": {},
   "source": [
    "## Pretty Print chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b69b95ee-fd2b-426c-a73a-aa6632d7a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Chain Structure:\n",
      "==================================================\n",
      "Chain Type: RunnableSequence\n",
      "Chain ID: 127414590194304\n",
      "\n",
      "Chain Components:\n",
      "------------------------------\n",
      "1. Prompt Component:\n",
      "   Type: ChatPromptTemplate\n",
      "   Messages: 2\n",
      "   Message 1: system\n",
      "     Content: You are a helpful assistant. Answer all questions to the bes...\n",
      "   Message 2: messagesplaceholder\n",
      "\n",
      "2. Model Component:\n",
      "   Type: ChatOpenAI\n",
      "   Model Name: gpt-3.5-turbo\n",
      "   Temperature: None\n",
      "   Max Tokens: None\n",
      "\n",
      "Chain Methods:\n",
      "--------------------\n",
      "Available methods: InputType, OutputType, abatch, abatch_as_completed, ainvoke, as_tool, assign, astream, astream_events, astream_log\n",
      "... and 58 more\n",
      "\n",
      "Raw Chain Object:\n",
      "--------------------\n",
      "first=ChatPromptTemplate(input_variables=[], optional_variables=['messages'], input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x73e21c3defc0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'messages': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant. Answer all questions to the best of your ability.'), additional_kwargs={}), MessagesPlaceholder(variable_name='messages', optional=True)]) middle=[] last=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x73e205ed2630>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x73e20509a990>, root_client=<openai.OpenAI object at 0x73e205ecb4d0>, root_async_client=<openai.AsyncOpenAI object at 0x73e205034c80>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "\n",
      "Compact Chain Summary:\n",
      "-------------------------\n",
      "ðŸ“ Prompt: 2 message templates\n",
      "ðŸ¤– Model: gpt-3.5-turbo\n",
      "ðŸ”— Chain: ChatPromptTemplate | ChatOpenAI\n",
      "\n",
      "Usage Example:\n",
      "---------------\n",
      "# To invoke the chain:\n",
      "response = chain.invoke({'messages': [('user', 'Hello!')]})\n",
      "# Or stream responses:\n",
      "for chunk in chain.stream({'messages': [('user', 'Hello!')]}): ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "\n",
    "# Pretty print the chain\n",
    "print(\"LangChain Chain Structure:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Chain Type: {type(chain).__name__}\")\n",
    "print(f\"Chain ID: {id(chain)}\")\n",
    "\n",
    "# Break down the chain components\n",
    "print(\"\\nChain Components:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# First component (prompt)\n",
    "print(\"1. Prompt Component:\")\n",
    "print(f\"   Type: {type(chain.first).__name__}\")\n",
    "print(f\"   Messages: {len(chain.first.messages)}\")\n",
    "for i, msg in enumerate(chain.first.messages):\n",
    "    msg_type = type(msg).__name__.replace('MessagePromptTemplate', '').lower()\n",
    "    print(f\"   Message {i+1}: {msg_type}\")\n",
    "    if hasattr(msg, 'prompt') and hasattr(msg.prompt, 'template'):\n",
    "        content = msg.prompt.template\n",
    "        if len(content) > 60:\n",
    "            content = content[:60] + \"...\"\n",
    "        print(f\"     Content: {content}\")\n",
    "\n",
    "# Second component (model)\n",
    "print(\"\\n2. Model Component:\")\n",
    "print(f\"   Type: {type(chain.last).__name__}\")\n",
    "print(f\"   Model Name: {getattr(chain.last, 'model_name', 'Not specified')}\")\n",
    "print(f\"   Temperature: {getattr(chain.last, 'temperature', 'Not specified')}\")\n",
    "print(f\"   Max Tokens: {getattr(chain.last, 'max_tokens', 'Not specified')}\")\n",
    "\n",
    "# Show available methods\n",
    "print(\"\\nChain Methods:\")\n",
    "print(\"-\" * 20)\n",
    "chain_methods = [method for method in dir(chain) if not method.startswith('_') and callable(getattr(chain, method))]\n",
    "print(f\"Available methods: {', '.join(chain_methods[:10])}\")\n",
    "if len(chain_methods) > 10:\n",
    "    print(f\"... and {len(chain_methods) - 10} more\")\n",
    "\n",
    "# Show the raw chain representation\n",
    "print(\"\\nRaw Chain Object:\")\n",
    "print(\"-\" * 20)\n",
    "print(chain)\n",
    "\n",
    "# Alternative: More compact representation\n",
    "print(\"\\nCompact Chain Summary:\")\n",
    "print(\"-\" * 25)\n",
    "print(f\"ðŸ“ Prompt: {len(chain.first.messages)} message templates\")\n",
    "print(f\"ðŸ¤– Model: {getattr(chain.last, 'model_name', 'ChatOpenAI')}\")\n",
    "print(f\"ðŸ”— Chain: {type(chain.first).__name__} | {type(chain.last).__name__}\")\n",
    "\n",
    "# Show how to use the chain\n",
    "print(\"\\nUsage Example:\")\n",
    "print(\"-\" * 15)\n",
    "print(\"# To invoke the chain:\")\n",
    "print(\"response = chain.invoke({'messages': [('user', 'Hello!')]})\")\n",
    "print(\"# Or stream responses:\")\n",
    "print(\"for chunk in chain.stream({'messages': [('user', 'Hello!')]}): ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb236fa-7870-4818-b534-c2de153608aa",
   "metadata": {},
   "source": [
    "## Finally call the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e362715-cc17-4ff4-91fb-1d5567276770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I said \"J'adore programmer,\" which means \"I love programming\" in French.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\n",
    "    \"messages\": [\n",
    "        (\"human\", \"Translate this sentence from English to French: I love programming.\"),\n",
    "        (\"ai\", \"J'adore programmer.\"),\n",
    "        (\"human\", \"What did you just say?\"),\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d292bc-4a96-4581-ba84-7be8ea04bf91",
   "metadata": {},
   "source": [
    "## A fuller example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "602849ee-8c27-4108-9743-5e2e8a8712af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Response\n",
      "===========\n",
      "Content: The translation of \"I love programming\" from English to French is \"J'aime la programmation.\"\n",
      "Type: AIMessage\n",
      "Metadata: {'token_usage': {'completion_tokens': 21, 'prompt_tokens': 39, 'total_tokens': 60, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BpxZsmt4vrt34QwjbI4T9vWnOopdi', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}\n",
      "Usage: {'input_tokens': 39, 'output_tokens': 21, 'total_tokens': 60, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Available attributes: additional_kwargs, content, example, id, invalid_tool_calls\n",
      "... and 12 more\n",
      "-----------\n",
      "content = The translation of \"I love programming\" from English to French is \"J'aime la programmation.\"\n",
      "Examples of letting AI do the translation:\n",
      "==================================================\n",
      "# Demo conversation structure:\n",
      "\n",
      "Translation Conversation Example\n",
      "================================\n",
      "1. ðŸ‘¤ Human:\n",
      "   Translate this sentence from English to French: I love programming.\n",
      "\n",
      "2. ðŸ¤– Ai:\n",
      "   J'adore programmer.\n",
      "\n",
      "3. ðŸ‘¤ Human:\n",
      "   What did you just say?\n",
      "\n",
      "4. ðŸ¤– Ai:\n",
      "   I said 'J'adore programmer,' which is the French translation of 'I love\n",
      "   programming' in English.\n",
      "\n",
      "\n",
      "============================================================\n",
      "HOW TO USE WITH REAL CHAIN RESPONSES:\n",
      "============================================================\n",
      "\n",
      "# Example 1: Single response\n",
      "response = chain.invoke({\n",
      "    \"messages\": [(\"human\", \"Translate 'hello' to French\")]\n",
      "})\n",
      "pretty_print_response(response, \"Translation Response\")\n",
      "\n",
      "# Example 2: Building conversation\n",
      "messages = [(\"human\", \"Translate 'hello' to French\")]\n",
      "response1 = chain.invoke({\"messages\": messages})\n",
      "pretty_print_response(response1, \"First Response\")\n",
      "\n",
      "# Add AI response to conversation\n",
      "messages.append((\"ai\", response1.content))\n",
      "messages.append((\"human\", \"What did you just say?\"))\n",
      "\n",
      "response2 = chain.invoke({\"messages\": messages})\n",
      "pretty_print_response(response2, \"Follow-up Response\")\n",
      "\n",
      "# Show full conversation\n",
      "pretty_print_conversation(messages + [(\"ai\", response2.content)], \"Complete Conversation\")\n",
      "\n",
      "\n",
      "========================================\n",
      "KEY BENEFITS:\n",
      "========================================\n",
      "âœ… Visual conversation flow with emojis\n",
      "âœ… Proper text wrapping for long responses\n",
      "âœ… Response metadata (tokens, timing, etc.)\n",
      "âœ… Easy debugging with attribute inspection\n",
      "âœ… Reusable for any LangChain response\n"
     ]
    }
   ],
   "source": [
    "def pretty_print_response(response, title=\"AI Response\"):\n",
    "    \"\"\"Pretty print a LangChain response object\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\" * len(title))\n",
    "    \n",
    "    # Handle different response types\n",
    "    if hasattr(response, 'content'):\n",
    "        # AIMessage object\n",
    "        print(f\"Content: {response.content}\")\n",
    "        print(f\"Type: {type(response).__name__}\")\n",
    "        \n",
    "        # Show additional fields if available\n",
    "        if hasattr(response, 'response_metadata') and response.response_metadata:\n",
    "            print(f\"Metadata: {response.response_metadata}\")\n",
    "        \n",
    "        if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
    "            print(f\"Usage: {response.usage_metadata}\")\n",
    "            \n",
    "    elif isinstance(response, str):\n",
    "        # String response\n",
    "        print(f\"Content: {response}\")\n",
    "        print(f\"Type: String\")\n",
    "    else:\n",
    "        # Other types\n",
    "        print(f\"Content: {response}\")\n",
    "        print(f\"Type: {type(response).__name__}\")\n",
    "        \n",
    "    # Show all attributes for debugging\n",
    "    if hasattr(response, '__dict__'):\n",
    "        attrs = [attr for attr in dir(response) if not attr.startswith('_') and not callable(getattr(response, attr))]\n",
    "        if attrs:\n",
    "            print(f\"Available attributes: {', '.join(attrs[:5])}\")\n",
    "            if len(attrs) > 5:\n",
    "                print(f\"... and {len(attrs) - 5} more\")\n",
    "    \n",
    "    print(\"-\" * len(title))\n",
    "    \n",
    "# Example 1: Let AI do the translation\n",
    "response = chain.invoke({\n",
    "    \"messages\": [\n",
    "        (\"human\", \"Translate this sentence from English to French: I love programming.\"),\n",
    "    ],\n",
    "})\n",
    "# AI responds: \"J'adore programmer.\"\n",
    "\n",
    "pretty_print_response(response)\n",
    "print(f\"content = {response.content}\")\n",
    "\n",
    "\n",
    "# Then if you want to continue the conversation:\n",
    "response2 = chain.invoke({\n",
    "    \"messages\": [\n",
    "        (\"human\", \"Translate this sentence from English to French: I love programming.\"),\n",
    "        (\"ai\", \"J'adore programmer.\"),\n",
    "        (\"human\", \"What did you just say?\"),\n",
    "    ],\n",
    "})\n",
    "# AI responds: \"I said 'J'adore programmer,' which means 'I love programming' in French.\"\n",
    "\n",
    "# Example 2: Multiple translations in one go\n",
    "response = chain.invoke({\n",
    "    \"messages\": [\n",
    "        (\"human\", \"Translate these sentences from English to French:\\n1. I love programming.\\n2. The weather is nice today.\\n3. Where is the library?\"),\n",
    "    ],\n",
    "})\n",
    "\n",
    "# Example 3: Interactive translation session\n",
    "response = chain.invoke({\n",
    "    \"messages\": [\n",
    "        (\"human\", \"I'm learning French. Can you help me translate some phrases?\"),\n",
    "        (\"ai\", \"Of course! I'd be happy to help you with French translations. What would you like to translate?\"),\n",
    "        (\"human\", \"How do you say 'I love programming' in French?\"),\n",
    "    ],\n",
    "})\n",
    "\n",
    "# Example 4: Translation with explanation\n",
    "response = chain.invoke({\n",
    "    \"messages\": [\n",
    "        (\"human\", \"Translate 'I love programming' to French and explain the grammar.\"),\n",
    "    ],\n",
    "})\n",
    "\n",
    "# Example 5: Building conversation from scratch\n",
    "# First call - just the translation request\n",
    "initial_response = chain.invoke({\n",
    "    \"messages\": [\n",
    "        (\"human\", \"Translate this sentence from English to French: I love programming.\"),\n",
    "    ],\n",
    "})\n",
    "\n",
    "# Second call - add the AI's response and ask follow-up\n",
    "# Note: You'd get the actual AI response from initial_response.content\n",
    "follow_up_response = chain.invoke({\n",
    "    \"messages\": [\n",
    "        (\"human\", \"Translate this sentence from English to French: I love programming.\"),\n",
    "        (\"ai\", initial_response.content),  # The actual AI translation\n",
    "        (\"human\", \"What did you just say?\"),\n",
    "    ],\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "def pretty_print_conversation(messages, title=\"Conversation Flow\"):\n",
    "    \"\"\"Pretty print a conversation history\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\" * len(title))\n",
    "    \n",
    "    for i, (role, content) in enumerate(messages, 1):\n",
    "        # Use emojis for visual clarity\n",
    "        emoji = \"ðŸ¤–\" if role == \"ai\" else \"ðŸ‘¤\" if role == \"human\" else \"âš™ï¸\"\n",
    "        role_display = role.capitalize()\n",
    "        \n",
    "        print(f\"{i}. {emoji} {role_display}:\")\n",
    "        # Wrap long content\n",
    "        if len(content) > 80:\n",
    "            words = content.split()\n",
    "            lines = []\n",
    "            current_line = []\n",
    "            current_length = 0\n",
    "            \n",
    "            for word in words:\n",
    "                if current_length + len(word) + 1 > 80:\n",
    "                    lines.append(\" \".join(current_line))\n",
    "                    current_line = [word]\n",
    "                    current_length = len(word)\n",
    "                else:\n",
    "                    current_line.append(word)\n",
    "                    current_length += len(word) + 1\n",
    "            \n",
    "            if current_line:\n",
    "                lines.append(\" \".join(current_line))\n",
    "            \n",
    "            for line in lines:\n",
    "                print(f\"   {line}\")\n",
    "        else:\n",
    "            print(f\"   {content}\")\n",
    "        print()\n",
    "\n",
    "# Demo the pretty printing functions\n",
    "print(\"Examples of letting AI do the translation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example with actual chain invocation (commented since we need real chain)\n",
    "print(\"# Demo conversation structure:\")\n",
    "demo_messages = [\n",
    "    (\"human\", \"Translate this sentence from English to French: I love programming.\"),\n",
    "    (\"ai\", \"J'adore programmer.\"),\n",
    "    (\"human\", \"What did you just say?\"),\n",
    "    (\"ai\", \"I said 'J'adore programmer,' which is the French translation of 'I love programming' in English.\")\n",
    "]\n",
    "\n",
    "pretty_print_conversation(demo_messages, \"Translation Conversation Example\")\n",
    "\n",
    "# Show how to use with real responses\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HOW TO USE WITH REAL CHAIN RESPONSES:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "# Example 1: Single response\n",
    "response = chain.invoke({\n",
    "    \"messages\": [(\"human\", \"Translate 'hello' to French\")]\n",
    "})\n",
    "pretty_print_response(response, \"Translation Response\")\n",
    "\n",
    "# Example 2: Building conversation\n",
    "messages = [(\"human\", \"Translate 'hello' to French\")]\n",
    "response1 = chain.invoke({\"messages\": messages})\n",
    "pretty_print_response(response1, \"First Response\")\n",
    "\n",
    "# Add AI response to conversation\n",
    "messages.append((\"ai\", response1.content))\n",
    "messages.append((\"human\", \"What did you just say?\"))\n",
    "\n",
    "response2 = chain.invoke({\"messages\": messages})\n",
    "pretty_print_response(response2, \"Follow-up Response\")\n",
    "\n",
    "# Show full conversation\n",
    "pretty_print_conversation(messages + [(\"ai\", response2.content)], \"Complete Conversation\")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"KEY BENEFITS:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"âœ… Visual conversation flow with emojis\")\n",
    "print(\"âœ… Proper text wrapping for long responses\") \n",
    "print(\"âœ… Response metadata (tokens, timing, etc.)\")\n",
    "print(\"âœ… Easy debugging with attribute inspection\")\n",
    "print(\"âœ… Reusable for any LangChain response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1130e1e9-9ac3-491f-bb9b-3891e2718ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
