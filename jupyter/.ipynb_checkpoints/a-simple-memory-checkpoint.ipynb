{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dbdf009-e881-4664-a78d-92877650ce64",
   "metadata": {},
   "source": [
    "## From Chapter 4 - a-simple-memory.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e9d6b00-4423-4e65-b90f-3ac1e48faf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt = input_variables=[] optional_variables=['messages'] input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x73e21c3defc0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={'messages': []} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant. Answer all questions to the best of         your ability.'), additional_kwargs={}), MessagesPlaceholder(variable_name='messages', optional=True)]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of         your ability.\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "print(f\"prompt = {prompt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4cf9db-4cfb-4caa-996f-f55be0ac2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"messages\": [\n",
    "        (\"human\", \"Translate this sentence from English to French: I love programming.\"),\n",
    "        (\"ai\", \"J'adore programmer.\"),\n",
    "        (\"human\", \"What did you just say?\"),\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133526b-9039-49bd-818c-792b510c1f71",
   "metadata": {},
   "source": [
    "## Pretty Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e33457a4-67a8-4b02-8b67-5ed50dfb1d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template Structure:\n",
      "==================================================\n",
      "Type: ChatPromptTemplate\n",
      "Number of messages: 2\n",
      "\n",
      "Message 1:\n",
      "  Type: SystemMessagePromptTemplate\n",
      "  Role: system\n",
      "  Content: You are a helpful assistant. Answer all questions to the best of your ability.\n",
      "  Available attributes: ['additional_kwargs', 'aformat', 'aformat_messages', 'construct', 'copy', 'dict', 'format', 'format_messages', 'from_orm', 'from_template', 'from_template_file', 'get_lc_namespace', 'input_variables', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'pretty_print', 'pretty_repr', 'prompt', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'update_forward_refs', 'validate']\n",
      "\n",
      "Message 2:\n",
      "  Type: MessagesPlaceholder\n",
      "  Role: messagesplaceholder\n",
      "  Content: variable_name='messages' optional=True\n",
      "  Available attributes: ['aformat_messages', 'construct', 'copy', 'dict', 'format_messages', 'from_orm', 'get_lc_namespace', 'input_variables', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'n_messages', 'optional', 'parse_file', 'parse_obj', 'parse_raw', 'pretty_print', 'pretty_repr', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'update_forward_refs', 'validate', 'variable_name']\n",
      "\n",
      "Prompt Object:\n",
      "------------------------------\n",
      "input_variables=[] optional_variables=['messages'] input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x73e21c3defc0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={'messages': []} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant. Answer all questions to the best of your ability.'), additional_kwargs={}), MessagesPlaceholder(variable_name='messages', optional=True)]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create a chat prompt template with system message and placeholder for conversation\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "# Pretty print the prompt structure\n",
    "print(\"Prompt Template Structure:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Type: {type(prompt).__name__}\")\n",
    "print(f\"Number of messages: {len(prompt.messages)}\")\n",
    "\n",
    "# Display each message template\n",
    "for i, message in enumerate(prompt.messages):\n",
    "    print(f\"\\nMessage {i + 1}:\")\n",
    "    print(f\"  Type: {type(message).__name__}\")\n",
    "    \n",
    "    # Get the role from the message type or __class__ name\n",
    "    if hasattr(message, '__class__'):\n",
    "        role = message.__class__.__name__.replace('MessagePromptTemplate', '').lower()\n",
    "        print(f\"  Role: {role}\")\n",
    "    \n",
    "    # Try different ways to get the content\n",
    "    if hasattr(message, 'prompt'):\n",
    "        if hasattr(message.prompt, 'template'):\n",
    "            print(f\"  Content: {message.prompt.template}\")\n",
    "        else:\n",
    "            print(f\"  Content: {message.prompt}\")\n",
    "    elif hasattr(message, 'template'):\n",
    "        print(f\"  Content: {message.template}\")\n",
    "    else:\n",
    "        print(f\"  Content: {message}\")\n",
    "    \n",
    "    # Show all available attributes for debugging\n",
    "    print(f\"  Available attributes: {[attr for attr in dir(message) if not attr.startswith('_')]}\")\n",
    "\n",
    "# Alternative: Use the prompt's pretty representation\n",
    "print(\"\\nPrompt Object:\")\n",
    "print(\"-\" * 30)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9065b7-f529-4940-bc2f-57f11894109f",
   "metadata": {},
   "source": [
    "`ChatPromptTemplate.from_messages` is like a conversation blueprint builder in LangChain. Let me break it down:\n",
    "\n",
    "## What it does\n",
    "It creates a structured template for chat conversations by defining different types of messages and their roles. Think of it like creating a script template for a play - you define who says what and when.\n",
    "\n",
    "## How it works\n",
    "```python\n",
    "ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant...\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "```\n",
    "\n",
    "Each tuple in the list represents a message with two parts:\n",
    "- **Role** (first element): Who's \"speaking\" - system, user, assistant, placeholder\n",
    "- **Content** (second element): What they say or a template for what they'll say\n",
    "\n",
    "## The roles explained\n",
    "Think of it like different actors in a conversation:\n",
    "\n",
    "- **\"system\"**: The director's instructions - sets the AI's behavior and personality\n",
    "- **\"user\"**: The human's messages \n",
    "- **\"assistant\"**: The AI's responses\n",
    "- **\"placeholder\"**: A slot that gets filled with actual conversation history later\n",
    "\n",
    "## Your specific example\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "```\n",
    "\n",
    "This creates a template that:\n",
    "1. **Always starts** with system instructions (like giving the AI its job description)\n",
    "2. **Has a placeholder** `{messages}` that gets replaced with the actual back-and-forth conversation\n",
    "\n",
    "## The analogy\n",
    "It's like creating a form letter template:\n",
    "- The system message is like the letterhead that's always the same\n",
    "- The placeholder is like \"Dear {name}\" - it gets filled in with real data later\n",
    "\n",
    "When you use this template, LangChain will:\n",
    "1. Keep the system message at the top\n",
    "2. Replace `{messages}` with the actual conversation history\n",
    "3. Send the complete formatted conversation to the AI model\n",
    "\n",
    "This pattern is super common because it lets you maintain consistent AI behavior (via system message) while handling dynamic conversations (via the placeholder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b9f06-dd16-4b5b-b7b1-dcd5360688ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
