{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e815ed3-45bf-47d5-963d-7b538d83f6b3",
   "metadata": {},
   "source": [
    "# Persistant Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a0995-a51e-4656-96dc-b2f5da65b0cc",
   "metadata": {},
   "source": [
    "## Get key tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e12904d-573b-456b-bca2-f6063b8592ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using python-dotenv:\n",
      "API Key: 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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Method 1: Using python-dotenv (recommended)\n",
    "# First install: pip install python-dotenv\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    \n",
    "    # Load .env file from home directory\n",
    "    dotenv_path = Path.home() / '.env'\n",
    "    load_dotenv(dotenv_path)\n",
    "    \n",
    "    # Now you can access environment variables\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "#    database_url = os.getenv('DATABASE_URL')\n",
    "    \n",
    "    print(\"Using python-dotenv:\")\n",
    "    print(f\"API Key: {api_key}\")\n",
    "#    print(f\"Database URL: {database_url}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed. Install with: pip install python-dotenv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87a2d2d1-986a-4841-af47-19f9cae3ed5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7b7e61bed370>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END, add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "def chatbot(state: State):\n",
    "    answer = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "builder.add_node(\"chatbot\", chatbot)\n",
    "builder.add_edge(START, \"chatbot\")\n",
    "builder.add_edge(\"chatbot\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06f259e8-d873-45c5-b332-6bbedcc00306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='hi, my name is Jack!', additional_kwargs={}, response_metadata={}, id='3fbdd0bd-029c-473e-a1a8-420677da981a'), AIMessage(content='Hello Jack! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 14, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bpz3Ggf1RcJbvuY9soK7OPzpr6gGL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--65b9d05b-25ae-48e3-9868-16f174b4ce11-0', usage_metadata={'input_tokens': 14, 'output_tokens': 10, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "{'messages': [HumanMessage(content='hi, my name is Jack!', additional_kwargs={}, response_metadata={}, id='3fbdd0bd-029c-473e-a1a8-420677da981a'), AIMessage(content='Hello Jack! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 14, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bpz3Ggf1RcJbvuY9soK7OPzpr6gGL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--65b9d05b-25ae-48e3-9868-16f174b4ce11-0', usage_metadata={'input_tokens': 14, 'output_tokens': 10, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}, id='61a3fde4-cf6a-4f8f-acaf-4e996775a1f6'), AIMessage(content='Your name is Jack!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 37, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bpz3GeO052fn0vO9iS4MbrsvBudcH', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--8c19f743-f0b0-4f33-8533-8a742ece54a0-0', usage_metadata={'input_tokens': 37, 'output_tokens': 5, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "Your name is Jack!\n"
     ]
    }
   ],
   "source": [
    "# Add persistence with MemorySaver\n",
    "graph = builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# Configure thread\n",
    "thread1 = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Run with persistence\n",
    "result_1 = graph.invoke({\"messages\": [HumanMessage(\"hi, my name is Jack!\")]}, thread1)\n",
    "print(result_1)\n",
    "\n",
    "result_2 = graph.invoke({\"messages\": [HumanMessage(\"what is my name?\")]}, thread1)\n",
    "print(result_2)\n",
    "\n",
    "# Get the AI's response from the messages list\n",
    "ai_message = result_2[\"messages\"][-1]  # Get last message\n",
    "print(ai_message.content)  # Now THIS has .content\n",
    "\n",
    "# Or more safely, get the last AI message specifically\n",
    "#for msg in reversed(result_2[\"messages\"]):\n",
    "#    if hasattr(msg, 'content') and msg.__class__.__name__ == 'AIMessage':\n",
    "#        print(msg.content)\n",
    "#        break\n",
    "\n",
    "# Get state\n",
    "#print(graph.get_state(thread1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d9bd6-483c-489e-9a2f-ac80bd2abf25",
   "metadata": {},
   "source": [
    "Great question! The system doesn't automatically \"know\" it's thread_1 - **you explicitly tell it** by creating that configuration dictionary.\n",
    "\n",
    "Here's what's happening:\n",
    "\n",
    "**The thread configuration is manual:**\n",
    "```python\n",
    "# You create and name the thread yourself\n",
    "thread1 = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "```\n",
    "\n",
    "**Think of it like file naming in Python:**\n",
    "```python\n",
    "# You decide what to call your file\n",
    "with open(\"my_data.txt\", \"w\") as f:  # You chose \"my_data.txt\"\n",
    "    f.write(\"Hello\")\n",
    "\n",
    "# Similarly with threads\n",
    "thread1 = {\"configurable\": {\"thread_id\": \"1\"}}  # You chose \"1\"\n",
    "```\n",
    "\n",
    "**The thread_id is just a label you pick** - it could be anything:\n",
    "```python\n",
    "# These are all valid thread configurations\n",
    "thread_alice = {\"configurable\": {\"thread_id\": \"alice_conversation\"}}\n",
    "thread_bob = {\"configurable\": {\"thread_id\": \"bob_session\"}}\n",
    "thread_main = {\"configurable\": {\"thread_id\": \"main_workflow\"}}\n",
    "thread_debug = {\"configurable\": {\"thread_id\": \"debug_run_123\"}}\n",
    "```\n",
    "\n",
    "**How it works in practice:**\n",
    "```python\n",
    "# When you invoke the graph, you pass the thread config\n",
    "result = graph.invoke(\n",
    "    {\"input\": \"Hello\"}, \n",
    "    config=thread1  # This tells the graph which \"memory slot\" to use\n",
    ")\n",
    "```\n",
    "\n",
    "**Analogy**: It's like having multiple notebooks on your desk. The `thread_id` is like writing a label on each notebook cover. When you want to continue a conversation, you tell the system \"grab the notebook labeled '1'\" - but you're the one who decided to call it \"1\" in the first place.\n",
    "\n",
    "The checkpointer uses this ID to separate different conversation threads or workflow instances from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ab9acb-eb62-4860-a0e5-f9ae48ec3f74",
   "metadata": {},
   "source": [
    "### Is this a python thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70cc020-80d8-4bde-ae4f-de93424594dc",
   "metadata": {},
   "source": [
    "**No, this is NOT a Python thread** - it's completely different! The name \"thread\" here is just LangGraph's terminology for a conversation or workflow session.\n",
    "\n",
    "**Python threads vs LangGraph threads:**\n",
    "\n",
    "**Python threads (actual threading):**\n",
    "```python\n",
    "import threading\n",
    "\n",
    "def worker():\n",
    "    print(\"Running in background\")\n",
    "\n",
    "# This creates an actual OS thread\n",
    "python_thread = threading.Thread(target=worker)\n",
    "python_thread.start()  # Runs concurrently\n",
    "```\n",
    "\n",
    "**LangGraph \"threads\" (conversation sessions):**\n",
    "```python\n",
    "# This is just a label for a conversation/workflow\n",
    "thread1 = {\"configurable\": {\"thread_id\": \"1\"}}  # Not a real thread!\n",
    "```\n",
    "\n",
    "**Better analogy**: Think of LangGraph \"threads\" like **chat rooms** or **conversation tabs**:\n",
    "```python\n",
    "# Like having multiple chat conversations open\n",
    "chat_with_alice = {\"configurable\": {\"thread_id\": \"alice\"}}\n",
    "chat_with_bob = {\"configurable\": {\"thread_id\": \"bob\"}}\n",
    "work_discussion = {\"configurable\": {\"thread_id\": \"work_chat\"}}\n",
    "\n",
    "# Each \"thread\" is just a separate conversation history\n",
    "graph.invoke({\"input\": \"Hi Alice\"}, config=chat_with_alice)\n",
    "graph.invoke({\"input\": \"Hey Bob\"}, config=chat_with_bob)\n",
    "```\n",
    "\n",
    "**What LangGraph \"threads\" actually are:**\n",
    "- Separate memory spaces for different conversations\n",
    "- Different workflow instances \n",
    "- Isolated state storage\n",
    "- Like having multiple save files in a game\n",
    "\n",
    "**Key difference**: Python threads run code simultaneously, but LangGraph threads just organize different conversation histories. No actual concurrent execution is happening - it's just a way to keep different interactions separate in memory.\n",
    "\n",
    "The confusing naming is unfortunate, but they're completely unrelated concepts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc0359-9127-4864-98aa-e00fff381ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
