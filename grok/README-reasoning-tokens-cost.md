**Reasoning tokens typically count as OUTPUT tokens** for billing purposes, similar to how OpenAI handles their o1 models.

## **Why Output Tokens:**

1. **Generated by the model** - Reasoning tokens are produced by the AI during inference, not provided by you
2. **Computational cost** - They require the same GPU/compute resources as visible output tokens
3. **Model processing** - The AI generates these tokens as part of creating your response

## **Billing Structure (typical):**
```
Input tokens:  21 × $X per 1K tokens
Output tokens: 826 × $Y per 1K tokens  // 563 visible + 263 reasoning
Total cost:    Input cost + Output cost
```

## **Industry Pattern:**
- **OpenAI o1**: Reasoning tokens billed as output tokens
- **Anthropic Claude**: No separate reasoning tokens (yet)
- **xAI Grok**: Likely follows same pattern as o1

## **Why This Matters:**

**Output tokens usually cost 3-4x more than input tokens**, so reasoning tokens can significantly impact your bill:

```
Example pricing:
Input:  $0.50 per 1M tokens
Output: $2.00 per 1M tokens

Your request:
Input cost:  21 × $0.50/1M = $0.0000105
Output cost: 826 × $2.00/1M = $0.001652
Total: ~$0.0017 (reasoning tokens = 32% of output cost!)
```

## **Best Practice:**
Check xAI's specific pricing documentation, but expect reasoning tokens to be billed as output tokens. This makes requests with heavy "thinking" more expensive than simple factual responses.

**Pro tip:** If cost is a concern, consider using models without reasoning tokens for simpler queries where deep thinking isn't needed!
